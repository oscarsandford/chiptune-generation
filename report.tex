\documentclass{article}
\usepackage[T1]{fontenc} % add special characters (e.g., umlaute)
\usepackage[utf8]{inputenc} % set utf-8 as default input encoding
\usepackage{ismir,amsmath,cite,url}
\usepackage{graphicx}
\usepackage{color}

% UNCOMMENT THE TWO LINES BELOW TO SEE LINE NUMBERS. I think it is better without the line numbers.
% \usepackage{lineno}
% \linenumbers

\title{Track Extension and Generation using Markov chains}

% Three addresses
% --------------
\threeauthors
  {Colson Demedeiros} {University of Victoria \\ {\tt cdemedeiros@uvic.ca}}
  {Jae Park} {University of Victoria \\ {\tt jaehyunpark@uvic.ca}}
  {Oscar Sandford} {University of Victoria \\ {\tt oscarsandford@uvic.ca}}

\sloppy
\begin{document}
\maketitle

\begin{abstract}
Write this last.
\end{abstract}

\section{Introduction}
Music is typically constructed by humans, for humans. However, machines are becoming more adept at revealing patterns in the way musicians craft their chords. 
This enables humans to create their own music, and then let the machine take over the task of composer. Our project aims to understand and implement a framework 
for simple 8-bit music generation through temporal inference techniques. In this paper, we explore previous implementations and existing literature regarding music 
generation, and present techniques for designing generative Markov models to create derivative musical works.

\section{Related Work}
\subsection{Markov Models}
Yanchenko and Mukherjee explored the use of Hidden Markov Models (HMMs) to compose classical music, finding proficiency in generating consonant harmonies, but 
lacking melodic progression \cite{yanchenko_2017}. Indeed, the models were found to learn the harmonic hidden states quite well, in some cases leading to overfitting. 
HMMs have also found use in chorale harmonization, where a given observable melody uses inference to derive hidden harmonies to complement it \cite{allan_2005}. 

Walter and Van Der Merwe's methods involve representing the chord duration, chord progression, and rhythm progression with first or higher-order Markov chains, whereas 
the overlaying melodic arc is represented by a HMM \cite{walter_2011}. This separation works well to reduce the processing power needed for music generation, but the 
independent learning of each component leads to less cohesive compositions. Generating music is generally done by sampling a statistical model \cite{conklin_2003}.
However, we want to create music that does not only simply replicate the training data, but also creates cohesive pieces in a more natural way. 

Shapiro and Huber's approach to music generation simply uses Markov chains, no hidden states \cite{shapiro_huber_2021}. In their work, the states represent sound 
objects with attributes such as pitch, octave, and duration. Their results show that human-composed pieces can be closely replicated using the simpler Markov chains. 
Further, they have attached their implementation in their paper. We will consider this work when constructing our own implementation.

CorrÃªa and Jungling suggest using Markov chains of different orders to predict classical music. By using stocahsic models to analyze different classcal songs and styles,
the computer can even captures subtle and intuitive features such as style of composer. Although this paper focuses on classcial music and its prediction, the authors stress
that its applicaition to other mucis genres should be straightforward. The only requirement is a MIDI file with a good quality, as it should be used for markov chains can 
properly estimate the music's pattern.

\subsection{Data Format}
The papers previously mentioned use the MIDI file format to write digital music. This format appears to be the standard for digital music creation \cite{midi_format}. 
One of MIDI's drawbacks is that it cannot store vocals \cite{cataltepe_2007}. This is of no concern to us, as we will only be attempting to generate instrumental 
compositions. Additionally, successful approaches to melody extraction from MIDI files \cite{ozcan_2005} make assurances that this will be an adequate medium for the 
music our models will generate. MusicXML is a standard file format for storing sheet music, just as MP3 is for recordings \cite{musicxml_2022}. As both are commonly used 
standards, we plan to use MusicXML for input data and write our output to MIDI files.

\section{Timeline}
Text here, probably a figure too. 

\section{Task Delegation}
In order to figure out the best approach and gather a plethora of sources, we are each looking at various sources related to music data parsing and music generation, 
from theoretical papers to Python libraries. Colson and Jae are finding classic 8-bit tracks that we will use to train our models on. Oscar has set up a GitHub repository 
to include written work as well as source code, and made outlines for the final report. 

Text

\section{Resources}

\subsection{Tools}
Any tools we'll use (e.g. Jupyter Notebooks, MIDI, MusicXML, Python libraries).

\subsection{Data Sets}
Data sets we'll use (e.g. 8-bit tracks to train/generate from).

 % END

% Add bibtext citations to the file `report_bibliography.bib`.
\bibliography{report_bibliography}
\end{document}
