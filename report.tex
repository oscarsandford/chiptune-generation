\documentclass{article}
\usepackage[T1]{fontenc} % add special characters (e.g., umlaute)
\usepackage[utf8]{inputenc} % set utf-8 as default input encoding
\usepackage{ismir,amsmath,cite,url}
\usepackage{graphicx}
\usepackage{color}

% UNCOMMENT THE TWO LINES BELOW TO SEE LINE NUMBERS. I think it is better without the line numbers.
% \usepackage{lineno}
% \linenumbers

\title{Track Extension and Generation using Markov chains}

% Three addresses
% --------------
\threeauthors
  {Colson Demedeiros} {University of Victoria \\ {\tt cdemedeiros@uvic.ca}}
  {Jae Park} {University of Victoria \\ {\tt jaehyunpark@uvic.ca}}
  {Oscar Sandford} {University of Victoria \\ {\tt oscarsandford@uvic.ca}}

\sloppy
\begin{document}
\maketitle

\begin{abstract}
Write this last.
\end{abstract}

\section{Introduction}
Music is typically constructed by humans, for humans. However, machines are becoming more adept at revealing patterns in the way musicians craft their chords. 
This enables humans to create their own music, and then let the machine take over the task of composer. Our project aims to understand and implement a framework 
for simple 8-bit music generation through temporal inference techniques. In this paper, we explore previous implementations and existing literature regarding music 
generation, and present techniques for designing generative Markov models to create derivative musical works.

\section{Related Work}
Yanchenko and Mukherjee explored the use of Hidden Markov Models (HMMs) to compose classical music, finding proficiency in generating consonant harmonies, but 
lacking melodic progression \cite{yanchenko_2017}. Indeed, the models were found to learn the harmonic hidden states quite well, in some cases leading to overfitting. 
HMMs have also found use in chorale harmonization, where a given observable melody uses inference to derive hidden harmonies to complement it \cite{allan_2005}. 

Shapiro and Huber's approach to music generation simply uses Markov chains, no hidden states \cite{shapiro_huber_2021}. In their work, the states represent sound 
objects with attributes such as pitch, octave, and duration. Their results show that human-composed pieces can be closely replicated using the simpler Markov chains. 
Further, they have attached their implementation in their paper. We will consider this work when constructing our own implementation.

The papers previously mentioned use the MIDI file format to write digital music. This format appears to be the standard for digital music creation \cite{midi_format}. 
Additionally, successful approaches to melody extraction from MIDI files \cite{ozcan_2005} make assurances that this will be an adequate medium for the music our models 
will generate. MusicXML is a standard file format for storing sheet music, just as MP3 is for recordings \cite{musicxml_2022}. As both are commonly used standards, we 
plan to use MusicXML for input data and write our output to MIDI files.

\section{Timeline}
Text here, probably a figure too. 

\section{Task Delegation}
In order to figure out the best approach and gather a plethora of sources, we are each looking at various sources related to music data parsing and music generation, 
from theoretical papers to Python libraries. Colson and Jae are finding classic 8-bit tracks that we will use to train our models on. Oscar has set up a GitHub repository 
to include written work as well as source code, and made outlines for the final report. 

Text

\section{Resources}

\subsection{Tools}
Any tools we'll use (e.g. Jupyter Notebooks, MIDI, MusicXML, Python libraries).

\subsection{Data Sets}
Data sets we'll use (e.g. 8-bit tracks to train/generate from).

 % END

% Add bibtext citations to the file `report_bibliography.bib`.
\bibliography{report_bibliography}
\end{document}
